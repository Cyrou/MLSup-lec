#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Dec  4 09:43:17 2018

@author: cyrilrousset
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.model_selection import train_test_split
import numpy as np
import random
import math as math
from numpy import linalg as LA

x=pd.read_csv('/Users/cyrilrousset/Desktop/data_set_4_X.csv')
y=pd.read_csv('/Users/cyrilrousset/Desktop/data_set_4_Y.csv')

#DefinitionKernel
def GKernel(x,y,sigma):
    return math.exp(-(LA.norm(x+(-1)*y)**2))/(2*sigma**2)

#TestKernel

a=[1,1]  
b=[0,0] 
sigma =2
print(GKernel(a,b,sigma))

#Q1-Plotage x2 fonction de x1
x1red=[]
x2red=[]
x1blue=[]
x2blue=[]

for i in range(99999):
    if y.iloc[i][0]==-1 :
        x1red.append(x.iloc[i][0])
        x2red.append(x.iloc[i][1])
    else:
        x1blue.append(x.iloc[i][0])
        x2blue.append(x.iloc[i][1])
        
plt.plot(x1blue,x2blue,'bo')
plt.plot(x1red,x2red,'ro')
plt.title('x2-x1')
plt.xlabel('x1')
plt.ylabel('x2')
plt.show

x1red.clear
x2red.clear
x1blue.clear
x2blue.clear

#2-TestKernel
s=0
r=np.random.uniform(-9,9,9)
for i in range(9):
    for j in range(9):
        s=s+r[i]*r[j]*GKernel(x.iloc[i],x.iloc[j],1)
print(s)







#3-Taux d'erreur normal
dataX=x
dataY=np.ravel(pd.read_csv('/Users/cyrilrousset/Desktop/data_set_4_Y.csv'))
mprime = 1000
eta = 0.5
T = mprime
mu=0
error_res = []
error_tab = []
for j in range(10): 
        w=np.array([[0, 0]])
        X_train, X_test, y_train, y_test = train_test_split(dataX,dataY, train_size=mprime/len(dataY), random_state=0)
        for t in range (T) :
            i = random.randint(0,len(X_train)-1) 
            v = 2*mu*w[t]
            if (  y_train[i]*np.dot(w[t],X_train.values[i])  < 1 ) :
                v = v - y_train[i]*X_train.values[i]
            temp = w[t]-eta*v
            w = np.append(w,[temp],axis = 0)
        w_res = w.mean(0)
        error = 0
        for i in range(len(X_test)-1):
          #  print('value =',y_test[i]*np.dot(w_res,X_test.values[i]))
            if ( y_test[i]*np.dot(w_res,X_test.values[i]) < 1 ) :
                error = error + ((1 - y_test[i]*np.dot(w_res,X_test.values[i])))/(len(X_test))
        error_tab.append(error)
error_mean = np.mean(error_tab)
print(error_mean)        
    

#4-Taux d'erreur SVD Kernel
dataX=x
dataY=np.ravel(pd.read_csv('/Users/cyrilrousset/Desktop/data_set_4_Y.csv'))
mprime = 1000
eta = 0.5
T = mprime
mu=0
sigmas=[0.001,0.01,0.1,1,10]
for sigma in sigmas:
    error_res = []
    error_tab = []
    for j in range(10): 
        w=np.array([[0, 0]])
        X_train, X_test, y_train, y_test = train_test_split(dataX,dataY, train_size=mprime/len(dataY), random_state=0)
        for t in range (T) :
            i = random.randint(0,len(X_train)-1) 
            v = 2*mu*w[t]
            if (  y_train[i]*GKernel(w[t],X_train.values[i],sigma)  < 1 ) :
                v = v - y_train[i]*X_train.values[i]
            temp = w[t]-eta*v
            w = np.append(w,[temp],axis = 0)
        w_res = w.mean(0)
        error = 0
        for i in range(len(X_test)-1):
          #  print('value =',y_test[i]*np.dot(w_res,X_test.values[i]))
            if ( y_test[i]*np.dot(w_res,X_test.values[i]) < 1 ) :
                error = error + ((1 - y_test[i]*GKernel(w_res,X_test.values[i],sigma)))/(len(X_test))
        error_tab.append(error)
    error_mean = np.mean(error_tab)
    print('sigma: {}'.format(sigma))   
    print('error: {}'.format(error_mean))   


    
    
    
#5-Comparaison avec k-means
y=np.ravel(y)
val = [1,2,3]
for k in val :
   X_train, X_test, y_train, y_test = train_test_split(x,y, train_size=0.8, random_state=0)
   clf = KNeighborsClassifier(n_neighbors=k)  
   clf.fit(X_train, y_train)
   Yres=clf.predict(X_test)
   sumVect=(Yres-y_test)/2
   sumVect=np.absolute(sumVect)
   error=np.sum(sumVect)/(len(sumVect))
   print(error)
